================================================================================
           SCRIPT THUYẾT TRÌNH CHI TIẾT - PHASE 2: GPU BASIC
                    Tối ưu hóa Autoencoder với GPU
================================================================================

PHẦN 1: GIỚI THIỆU PHASE 2
================================================================================

Xin chào thầy/cô và các bạn,

Phần này em xin trình bày về Phase 2 - GPU Basic, đây là bước đầu tiên chuyển 
từ CPU sang GPU để tăng tốc độ xử lý.

1.1. BỐI CẢNH VÀ ĐỘNG LỰC
--------------------------
Sau khi hoàn thành Phase 1 trên CPU, nhóm em gặp phải vấn đề:

VẤN ĐỀ:
- Xử lý 60,000 ảnh CIFAR-10 mất 300-600 giây (5-10 phút!)
- Quá chậm cho ứng dụng thực tế
- Yêu cầu đề bài: phải dưới 20 giây

NGUYÊN NHÂN:
- CPU chỉ có 4-16 cores
- Xử lý tuần tự, không tận dụng được tính song song của Deep Learning
- Mỗi ảnh phải chờ ảnh trước xử lý xong

GIẢI PHÁP:
→ Chuyển sang GPU để tận dụng hàng nghìn cores xử lý song song!

1.2. MỤC TIÊU PHASE 2
----------------------
Nhóm em đặt ra 3 mục tiêu chính:

MỤC TIÊU 1: Chuyển đổi code từ CPU sang GPU
- Viết lại tất cả operations bằng CUDA kernels
- Quản lý bộ nhớ GPU (allocation, transfer)
- Đảm bảo kết quả đúng như Phase 1

MỤC TIÊU 2: Đạt yêu cầu về thời gian
- Xử lý 60,000 ảnh trong dưới 20 giây
- Tăng tốc ít nhất 15-20 lần so với CPU

MỤC TIÊU 3: Hiểu rõ GPU programming
- Nắm vững CUDA programming model
- Hiểu về threads, blocks, grids
- Làm nền tảng cho Phase 3 tối ưu hóa sâu hơn


================================================================================
PHẦN 2: TẠI SAO GPU NHANH HƠN CPU?
================================================================================

Để hiểu Phase 2, trước tiên phải hiểu tại sao GPU lại phù hợp với Deep Learning.

2.1. SO SÁNH KIẾN TRÚC CPU vs GPU
----------------------------------

CPU (Central Processing Unit):
┌─────────────────────────────────────┐
│  CPU - "Bộ não đa năng"             │
│                                     │
│  ┌────┐ ┌────┐ ┌────┐ ┌────┐      │
│  │Core│ │Core│ │Core│ │Core│      │
│  │ 1  │ │ 2  │ │ 3  │ │ 4  │      │
│  └────┘ └────┘ └────┘ └────┘      │
│                                     │
│  - Mỗi core RẤT MẠNH               │
│  - Xử lý logic phức tạp            │
│  - 4-16 cores                      │
└─────────────────────────────────────┘

GPU (Graphics Processing Unit):
┌─────────────────────────────────────┐
│  GPU - "Quân đội công nhân"         │
│                                     │
│  [Core][Core][Core]...[Core]       │
│  [Core][Core][Core]...[Core]       │
│  [Core][Core][Core]...[Core]       │
│  ... (hàng nghìn cores)            │
│                                     │
│  - Mỗi core ĐỠN GIẢN hơn           │
│  - Xử lý tính toán số học          │
│  - 1000-10000+ cores               │
└─────────────────────────────────────┘

2.2. MASSIVE PARALLELISM - GIẢI THÍCH CHI TIẾT
-----------------------------------------------

Hãy tưởng tượng bạn cần sơn 1000 cái ghế:

CÁCH LÀM CỦA CPU:
- Thuê 4 thợ chuyên nghiệp
- Mỗi thợ sơn 250 cái ghế
- Mỗi cái ghế mất 1 phút
- Tổng thời gian: 250 phút = 4 giờ 10 phút

CÁCH LÀM CỦA GPU:
- Thuê 1000 thợ (mỗi người kém chuyên nghiệp hơn)
- Mỗi thợ sơn 1 cái ghế
- Mỗi cái ghế mất 1.5 phút (chậm hơn chút)
- Tổng thời gian: 1.5 phút!

→ Nhanh hơn 166 lần!

ÁP DỤNG VÀO DEEP LEARNING:

Xử lý 1 ảnh 32×32 với Conv1 (256 channels):

CPU (Phase 1):
- 4-16 cores xử lý tuần tự
- Core 1: Tính channel 0, 1, 2, 3, ...
- Core 2: Tính channel 64, 65, 66, ...
- Phải tính 32×32×256 = 262,144 pixels
- Thời gian: ~500ms/ảnh

GPU (Phase 2):
- 262,144 threads chạy ĐỒNG THỜI
- Thread 0: Tính pixel (0,0) channel 0
- Thread 1: Tính pixel (0,1) channel 0
- Thread 262,143: Tính pixel (31,31) channel 255
- Tất cả chạy song song!
- Thời gian: ~0.25ms/ảnh

→ Nhanh hơn 2000 lần cho 1 ảnh!

2.3. MEMORY BANDWIDTH - GIẢI THÍCH CHI TIẾT
--------------------------------------------

Memory Bandwidth (Băng thông bộ nhớ) là gì?
- Là tốc độ truyền dữ liệu giữa RAM và chip xử lý
- Đơn vị: GB/s (Gigabyte per second)
- Càng cao càng tốt

HÌNH DUNG BẰNG ỐNG NƯỚC:

CPU RAM:
┌──────────┐     ╔═══╗     ┌──────────┐
│   RAM    │────▶║ ▓ ║────▶│   CPU    │
│  (DDR4)  │     ╚═══╝     │  Cores   │
└──────────┘    ~50 GB/s   └──────────┘
                Ống nhỏ

GPU VRAM:
┌──────────┐     ╔═════╗   ┌──────────┐
│  VRAM    │────▶║ ▓▓▓ ║──▶│   GPU    │
│ (GDDR6)  │     ╚═════╝   │  Cores   │
└──────────┘   ~300-900    └──────────┘
               GB/s
               Ống to gấp 6-18 lần!

TẠI SAO QUAN TRỌNG TRONG DEEP LEARNING?

Ví dụ Conv1 cần đọc:
- Input: 32×32×3 = 3,072 floats = 12 KB
- Weights: 256×3×3×3 = 6,912 floats = 27 KB
- Tổng: ~39 KB cần đọc

Với 60,000 ảnh:
- Tổng dữ liệu: 39 KB × 60,000 = 2.3 GB

Thời gian đọc:
- CPU (50 GB/s): 2.3 GB / 50 = 0.046 giây (chỉ riêng đọc!)
- GPU (500 GB/s): 2.3 GB / 500 = 0.0046 giây

Nhưng thực tế phải đọc/ghi nhiều lần hơn:
- Mỗi layer đọc input, ghi output
- 13 layers → đọc/ghi 13 lần
- Băng thông cao = tiết kiệm thời gian rất nhiều!

2.4. SPECIALIZED HARDWARE - GIẢI THÍCH CHI TIẾT
------------------------------------------------

CPU được thiết kế cho:
✓ Xử lý logic phức tạp (if-else, switch-case)
✓ Branch prediction (dự đoán nhánh)
✓ Out-of-order execution (thực thi không theo thứ tự)
✓ Đa nhiệm (chạy nhiều chương trình)
✓ Linh hoạt, đa năng

GPU được thiết kế cho:
✓ Tính toán số học (float operations)
✓ Nhân ma trận (matrix multiplication)
✓ Xử lý vector (SIMD - Single Instruction Multiple Data)
✓ Đồ họa 3D
✓ Deep Learning

VÍ DỤ CỤ THỂ: NHÂN 2 MA TRẬN 1000×1000

CPU:
- Dùng 4 cores
- Mỗi core tính 250 hàng
- Mỗi phần tử: 1000 phép nhân-cộng
- Thời gian: ~1-2 giây

GPU:
- Dùng 1000 threads
- Mỗi thread tính 1 hàng
- Song song hoàn toàn
- Có hardware chuyên (Tensor Cores trên GPU mới)
- Thời gian: ~10-50ms

→ Nhanh hơn 20-200 lần!

DEEP LEARNING LÀ GÌ?
- Chủ yếu là nhân ma trận!
- Convolution = Im2Col + Matrix Multiplication
- Fully Connected = Matrix Multiplication
- Attention = Matrix Multiplication

→ GPU là lựa chọn hoàn hảo!


================================================================================
PHẦN 3: DIRECT CONVOLUTION - CÁCH HOẠT ĐỘNG
================================================================================

3.1. CONVOLUTION LÀ GÌ? - ÔN LẠI KHÁI NIỆM
-------------------------------------------

Convolution (Tích chập) là phép toán cốt lõi trong CNN.

HÌNH DUNG:
- Có 1 ảnh input (ví dụ 32×32)
- Có 1 kernel/filter (ví dụ 3×3)
- Trượt kernel trên ảnh
- Tại mỗi vị trí: nhân từng phần tử rồi cộng lại
- Được 1 số → 1 pixel output

VÍ DỤ MINH HỌA:

Input (5×5):
┌───┬───┬───┬───┬───┐
│ 1 │ 2 │ 3 │ 4 │ 5 │
├───┼───┼───┼───┼───┤
│ 6 │ 7 │ 8 │ 9 │10 │
├───┼───┼───┼───┼───┤
│11 │12 │13 │14 │15 │
├───┼───┼───┼───┼───┤
│16 │17 │18 │19 │20 │
├───┼───┼───┼───┼───┤
│21 │22 │23 │24 │25 │
└───┴───┴───┴───┴───┘

Kernel (3×3):
┌───┬───┬───┐
│ 1 │ 0 │-1 │
├───┼───┼───┤
│ 2 │ 0 │-2 │
├───┼───┼───┤
│ 1 │ 0 │-1 │
└───┴───┴───┘

Tính pixel output tại vị trí (1,1):
- Lấy vùng 3×3 xung quanh (1,1):
  ┌───┬───┬───┐
  │ 1 │ 2 │ 3 │
  ├───┼───┼───┤
  │ 6 │ 7 │ 8 │
  ├───┼───┼───┤
  │11 │12 │13 │
  └───┴───┴───┘

- Nhân từng phần tử với kernel:
  1×1 + 2×0 + 3×(-1) +
  6×2 + 7×0 + 8×(-2) +
  11×1 + 12×0 + 13×(-1)
  
  = 1 + 0 - 3 + 12 + 0 - 16 + 11 + 0 - 13
  = -8

- Output[1,1] = -8

Lặp lại cho tất cả vị trí → được output 3×3 (với padding=0)

3.2. DIRECT CONVOLUTION TRÊN GPU - Ý TƯỞNG
-------------------------------------------

Ý tưởng đơn giản:
→ Mỗi thread GPU tính 1 pixel output!

CÁCH TỔ CHỨC THREADS:

Grid (lưới threads):
- Dimension X: Chiều rộng output (W)
- Dimension Y: Chiều cao output (H)
- Dimension Z: Số channels output (C_out)

Block (khối threads):
- Kích thước: 16×16 threads
- Mỗi block xử lý 1 vùng 16×16 pixels

Ví dụ với Conv1 (32×32×3 → 32×32×256):
- Grid: (2, 2, 256)
  + 2 = ceil(32/16) blocks theo chiều X
  + 2 = ceil(32/16) blocks theo chiều Y
  + 256 blocks theo chiều Z (mỗi channel 1 block)
- Block: (16, 16, 1)
- Tổng threads: 2 × 2 × 256 × 16 × 16 = 262,144 threads!

3.3. DIRECT CONVOLUTION - TỪNG BƯỚC CHI TIẾT
---------------------------------------------

Hãy theo dõi 1 thread cụ thể để hiểu cách hoạt động:

THREAD (x=10, y=15, z=0):

BƯỚC 1: Xác định nhiệm vụ
- Thread này tính pixel output tại vị trí (10, 15) của channel 0
- Tọa độ toàn cục:
  + x = blockIdx.x × 16 + threadIdx.x = 0 × 16 + 10 = 10
  + y = blockIdx.y × 16 + threadIdx.y = 0 × 16 + 15 = 15
  + channel = blockIdx.z = 0

BƯỚC 2: Xác định vùng cần đọc trên input
- Kernel 3×3 → cần vùng 3×3 xung quanh (10, 15)
- Vùng: từ (9, 14) đến (11, 16)
- Với 3 input channels (R, G, B)
- Tổng: 3×3×3 = 27 giá trị cần đọc

BƯỚC 3: Đọc input và weights từ global memory
Đọc input:
- input[R, 9, 14], input[R, 9, 15], input[R, 9, 16]
- input[R, 10, 14], input[R, 10, 15], input[R, 10, 16]
- input[R, 11, 14], input[R, 11, 15], input[R, 11, 16]
- Tương tự cho G và B
→ 27 lần đọc từ global memory

Đọc weights:
- weight[channel=0, input_ch=R, ky=0, kx=0]
- weight[channel=0, input_ch=R, ky=0, kx=1]
- ...
→ 27 lần đọc weights

BƯỚC 4: Tính toán
sum = 0
for input_channel in [R, G, B]:
    for ky in [-1, 0, 1]:
        for kx in [-1, 0, 1]:
            sum += input[input_channel, 10+ky, 15+kx] × 
                   weight[0, input_channel, ky+1, kx+1]

BƯỚC 5: Ghi kết quả
- output[channel=0, y=15, x=10] = sum
- 1 lần ghi vào global memory

TỔNG CỘNG:
- 27 lần đọc input
- 27 lần đọc weights
- 27 phép nhân
- 26 phép cộng
- 1 lần ghi output

3.4. VẤN ĐỀ 1: ĐỌC DỮ LIỆU NHIỀU LẦN
-------------------------------------

Đây là vấn đề NGHIÊM TRỌNG nhất của Direct Convolution!

PHÂN TÍCH VẤN ĐỀ:

Xét 2 threads lân cận:
- Thread A: tính pixel (10, 15)
- Thread B: tính pixel (11, 15)

Thread A đọc vùng:
┌────┬────┬────┐
│9,14│9,15│9,16│
├────┼────┼────┤
│10,14│10,15│10,16│  ← Vùng của Thread A
├────┼────┼────┤
│11,14│11,15│11,16│
└────┴────┴────┘

Thread B đọc vùng:
┌────┬────┬────┐
│10,14│10,15│10,16│
├────┼────┼────┤
│11,14│11,15│11,16│  ← Vùng của Thread B
├────┼────┼────┤
│12,14│12,15│12,16│
└────┴────┴────┘

VÙNG CHỒNG LẤN:
- 6 pixels bị đọc bởi CẢ 2 threads!
- Pixel (10,15) nằm trong vùng của 9 threads khác nhau
- Bị đọc 9 lần từ global memory!

TÍNH TOÁN CỤ THỂ:

Với Conv1 (32×32×256):
- Mỗi pixel output cần đọc 27 values
- Có 32×32×256 = 262,144 pixels output
- Tổng số lần đọc: 262,144 × 27 = 7,077,888 lần đọc!

Nhưng input chỉ có:
- 32×32×3 = 3,072 pixels
- Mỗi pixel bị đọc trung bình: 7,077,888 / 3,072 ≈ 2,304 lần!

HẬU QUẢ:
- Global memory latency: ~400-800 cycles
- Đọc 1 lần: ~400 cycles
- Đọc 2,304 lần: ~921,600 cycles!
- Cực kỳ chậm!

3.5. VẤN ĐỀ 2: KHÔNG TẬN DỤNG SHARED MEMORY
--------------------------------------------

SHARED MEMORY LÀ GÌ?

Shared Memory là bộ nhớ đặc biệt trên GPU:
- Nằm trên chip, rất gần các cores
- Nhanh hơn global memory 100 lần
- Latency: ~20-30 cycles (vs ~400-800 cycles)
- Nhưng nhỏ: chỉ ~48-96 KB per block

SO SÁNH:

┌─────────────────────────────────────┐
│         GLOBAL MEMORY               │
│  - Lớn: vài GB                      │
│  - Chậm: 400-800 cycles             │
│  - Xa: ngoài chip                   │
│  - Giống: Ổ cứng                    │
└─────────────────────────────────────┘
           ↕ (chậm)
┌─────────────────────────────────────┐
│         SHARED MEMORY               │
│  - Nhỏ: 48-96 KB                    │
│  - Nhanh: 20-30 cycles              │
│  - Gần: trên chip                   │
│  - Giống: RAM                       │
└─────────────────────────────────────┘
           ↕ (rất nhanh)
┌─────────────────────────────────────┐
│         GPU CORES                   │
└─────────────────────────────────────┘

TẠI SAO DIRECT CONVOLUTION KHÔNG DÙNG ĐƯỢC?

Vấn đề:
- Mỗi thread xử lý 1 pixel độc lập
- Thread A không biết Thread B cần gì
- Không có cơ chế chia sẻ dữ liệu
- Không thể load dữ liệu chung vào shared memory

Ví dụ:
- Thread A cần pixels (9,14) đến (11,16)
- Thread B cần pixels (10,14) đến (12,16)
- Nếu dùng shared memory:
  + Load pixels (9,14) đến (12,16) vào shared memory (1 lần)
  + Thread A và B đều đọc từ shared memory (nhanh!)
- Nhưng Direct Convolution không có cơ chế này!

KẾT QUẢ:
- Lãng phí tốc độ của shared memory
- Mọi thread đều đọc từ global memory chậm chạp

3.6. VẤN ĐỀ 3: KHÔNG TỐI ƯU CHO GPU
------------------------------------

GPU HIỆN ĐẠI CÓ GÌ ĐẶC BIỆT?

Tensor Cores (trên GPU Volta, Turing, Ampere):
- Hardware chuyên dụng cho nhân ma trận
- Có thể nhân 2 ma trận 4×4 trong 1 cycle!
- Nhanh hơn ALU thường 10-100 lần

Ví dụ:
- Nhân ma trận [256, 27] × [27, 1024]:
  + Dùng Tensor Cores: ~0.1ms
  + Dùng ALU thường: ~10ms
  + Chênh lệch 100 lần!

VẤN ĐỀ VỚI DIRECT CONVOLUTION:

Direct Convolution KHÔNG phải dạng ma trận:
- Mỗi thread tính độc lập
- Không có phép nhân ma trận rõ ràng
- Không thể dùng Tensor Cores!

So sánh:
┌────────────────────────────────────┐
│   DIRECT CONVOLUTION               │
│   - Mỗi thread: 27 phép nhân-cộng  │
│   - Dùng ALU thường                │
│   - Không dùng Tensor Cores        │
│   - Thời gian: ~70ms (Conv1)       │
└────────────────────────────────────┘

┌────────────────────────────────────┐
│   IM2COL + GEMM (Phase 3)          │
│   - Chuyển thành nhân ma trận      │
│   - Dùng Tensor Cores              │
│   - Thời gian: ~0.1ms (Conv1)      │
│   - Nhanh hơn 700 lần!             │
└────────────────────────────────────┘

KẾT LUẬN:
Direct Convolution không tận dụng được phần cứng GPU!


================================================================================
PHẦN 4: CÁC OPERATIONS KHÁC
================================================================================

Ngoài Convolution, mạng Autoencoder còn cần 3 operations khác.

4.1. RELU - HÀM KÍCH HOẠT
--------------------------

RELU LÀ GÌ?

ReLU = Rectified Linear Unit
Công thức: f(x) = max(0, x)

Nghĩa là:
- Nếu x < 0 → output = 0
- Nếu x ≥ 0 → output = x

VÍ DỤ:
Input:  [-2.5, -1.0, -0.3, 0, 0.5, 1.2, 3.7]
Output: [0,    0,    0,    0, 0.5, 1.2, 3.7]
         ↑     ↑     ↑     ↑
      âm→0  âm→0  âm→0  0→0

BIỂU ĐỒ:
    output
      ↑
    3 │         ╱
    2 │       ╱
    1 │     ╱
    0 │───╱─────────→ input
   -1 │ -2  -1  0  1  2  3
   
TẠI SAO CẦN RELU?

1. Thêm tính PHI TUYẾN:
   - Không có ReLU: mạng chỉ là phép nhân ma trận liên tiếp
   - Nhiều phép nhân ma trận = 1 phép nhân ma trận
   - Không học được pattern phức tạp!
   
   Ví dụ:
   y = W3 × (W2 × (W1 × x))  (không có ReLU)
   = (W3 × W2 × W1) × x
   = W_combined × x
   → Chỉ là 1 phép biến đổi tuyến tính!

2. Với ReLU:
   y = W3 × ReLU(W2 × ReLU(W1 × x))
   → Không thể gộp lại
   → Học được pattern phi tuyến phức tạp

3. Đơn giản và hiệu quả:
   - Tính toán rất nhanh (chỉ so sánh với 0)
   - Không bị vanishing gradient như Sigmoid
   - Hội tụ nhanh hơn

CÁCH GPU XỬ LÝ:

Mỗi thread xử lý 1 số:
- Thread 0: max(-2.5, 0) = 0
- Thread 1: max(-1.0, 0) = 0
- Thread 2: max(-0.3, 0) = 0
- Thread 3: max(0, 0) = 0
- Thread 4: max(0.5, 0) = 0.5
- Thread 5: max(1.2, 0) = 1.2
- Thread 6: max(3.7, 0) = 3.7

Với Conv1 output (32×32×256):
- 262,144 threads
- Mỗi thread: 1 phép so sánh
- Rất đơn giản, rất nhanh!
- Thời gian: ~4ms

4.2. MAXPOOL - GIẢM KÍCH THƯỚC
-------------------------------

MAXPOOL LÀ GÌ?

MaxPool = Maximum Pooling
- Chia ảnh thành các vùng 2×2
- Lấy giá trị MAX trong mỗi vùng
- Giảm kích thước xuống 1/2

VÍ DỤ CHI TIẾT:

Input (4×4):
┌────┬────┬────┬────┐
│ 1.2│ 0.5│ 2.1│ 1.8│
├────┼────┼────┼────┤
│ 3.1│ 2.0│ 0.9│ 1.5│
├────┼────┼────┼────┤
│ 0.8│ 1.3│ 2.7│ 3.2│
├────┼────┼────┼────┤
│ 1.9│ 0.6│ 1.1│ 2.4│
└────┴────┴────┴────┘

Chia thành 4 vùng 2×2:

Vùng 1 (top-left):     Vùng 2 (top-right):
┌────┬────┐            ┌────┬────┐
│ 1.2│ 0.5│            │ 2.1│ 1.8│
├────┼────┤            ├────┼────┤
│ 3.1│ 2.0│            │ 0.9│ 1.5│
└────┴────┘            └────┴────┘
max = 3.1              max = 2.1

Vùng 3 (bottom-left):  Vùng 4 (bottom-right):
┌────┬────┐            ┌────┬────┐
│ 0.8│ 1.3│            │ 2.7│ 3.2│
├────┼────┤            ├────┼────┤
│ 1.9│ 0.6│            │ 1.1│ 2.4│
└────┴────┘            └────┴────┘
max = 1.9              max = 3.2

Output (2×2):
┌────┬────┐
│ 3.1│ 2.1│
├────┼────┤
│ 1.9│ 3.2│
└────┴────┘

TẠI SAO CẦN MAXPOOL?

1. GIẢM KÍCH THƯỚC:
   - 32×32 → 16×16: giảm 75% dữ liệu!
   - Giảm số lượng tham số
   - Tăng tốc độ tính toán

2. TĂNG RECEPTIVE FIELD:
   - Receptive field = vùng ảnh gốc mà 1 neuron "nhìn thấy"
   - Sau MaxPool: mỗi pixel "nhìn" vùng rộng hơn
   - Học được features ở nhiều scales

3. TRANSLATION INVARIANCE:
   - Bất biến với vị trí
   - Ví dụ: mèo ở góc trái hay góc phải → vẫn nhận ra
   - MaxPool giúp mạng không quá nhạy cảm với vị trí

CÁCH GPU XỬ LÝ:

Với MaxPool từ 32×32 → 16×16:
- 16×16 = 256 vùng 2×2
- 256 threads (1 thread cho mỗi vùng)

Thread (x=5, y=7):
1. Xác định vùng 2×2 trên input:
   - Top-left: (10, 14)  [vì x×2=10, y×2=14]
   - Vùng: (10,14), (10,15), (11,14), (11,15)

2. Đọc 4 giá trị:
   - val1 = input[10, 14]
   - val2 = input[10, 15]
   - val3 = input[11, 14]
   - val4 = input[11, 15]

3. Tìm max:
   - max_val = max(val1, val2, val3, val4)

4. Ghi output:
   - output[5, 7] = max_val

Thời gian: ~4ms (rất nhanh!)

4.3. UPSAMPLE - TĂNG KÍCH THƯỚC
--------------------------------

UPSAMPLE LÀ GÌ?

Upsample = Upsampling
- Tăng kích thước ảnh lên gấp đôi
- Mỗi pixel → 4 pixels giống nhau
- Phương pháp: Nearest Neighbor

VÍ DỤ CHI TIẾT:

Input (2×2):
┌────┬────┐
│ 2.5│ 1.3│
├────┼────┤
│ 0.8│ 3.7│
└────┴────┘

Output (4×4):
┌────┬────┬────┬────┐
│ 2.5│ 2.5│ 1.3│ 1.3│  ← Hàng 1: copy từ hàng 1 input
├────┼────┼────┼────┤
│ 2.5│ 2.5│ 1.3│ 1.3│  ← Hàng 2: copy từ hàng 1 input
├────┼────┼────┼────┤
│ 0.8│ 0.8│ 3.7│ 3.7│  ← Hàng 3: copy từ hàng 2 input
├────┼────┼────┼────┤
│ 0.8│ 0.8│ 3.7│ 3.7│  ← Hàng 4: copy từ hàng 2 input
└────┴────┴────┴────┘

HÌNH DUNG:

Input pixel (0,0) = 2.5
→ Output pixels:
  (0,0) = 2.5
  (0,1) = 2.5
  (1,0) = 2.5
  (1,1) = 2.5

TẠI SAO CẦN UPSAMPLE?

Trong Autoencoder:
- Encoder: 32×32 → 16×16 → 8×8 (giảm dần)
- Latent: 8×8 (bottleneck)
- Decoder: 8×8 → 16×16 → 32×32 (tăng dần)

Upsample giúp:
- Khôi phục kích thước gốc
- Tái tạo ảnh từ latent space
- Đơn giản, nhanh

CÁCH GPU XỬ LÝ:

Với Upsample từ 16×16 → 32×32:
- 32×32 = 1024 pixels output
- 1024 threads (1 thread cho mỗi pixel output)

Thread (x=10, y=15):
1. Tính vị trí pixel input tương ứng:
   - input_x = x / 2 = 10 / 2 = 5
   - input_y = y / 2 = 15 / 2 = 7

2. Đọc giá trị:
   - val = input[5, 7]

3. Ghi output:
   - output[10, 15] = val

Rất đơn giản! Thời gian: ~4ms


================================================================================
PHẦN 5: KẾT QUẢ VÀ ĐÁNH GIÁ PHASE 2
================================================================================

5.1. BENCHMARK - KẾT QUẢ THỰC NGHIỆM
-------------------------------------

THIẾT LẬP:
- GPU: NVIDIA Tesla T4 (2560 CUDA cores)
- Dataset: CIFAR-10 (60,000 ảnh training)
- Kích thước ảnh: 32×32×3
- Batch size: 1 (xử lý từng ảnh)

KẾT QUẢ:

Thời gian xử lý 60,000 ảnh:
- Phase 1 (CPU): ~400 giây
- Phase 2 (GPU): ~18 giây
- Speedup: 22×
- ✓ ĐẠT yêu cầu < 20 giây!

Thời gian trung bình mỗi ảnh:
- Phase 1 (CPU): ~6.7 ms/ảnh
- Phase 2 (GPU): ~0.3 ms/ảnh
- Nhanh hơn 22 lần!

5.2. PHÂN TÍCH TIMING CHI TIẾT (1 ảnh)
---------------------------------------

Chạy test với 1 ảnh để xem timing từng layer:

FORWARD PASS (1 ảnh):
┌──────────────┬────────────┬────────────┐
│ LAYER        │ TIME (ms)  │ PERCENTAGE │
├──────────────┼────────────┼────────────┤
│ Conv1        │ 70.20      │ 85.3%      │ ← BOTTLENECK!
│ ReLU1        │ 4.09       │ 5.0%       │
│ MaxPool1     │ 3.90       │ 4.7%       │
│ Conv2        │ 0.007      │ 0.01%      │
│ ReLU2        │ 0.008      │ 0.01%      │
│ MaxPool2     │ 0.007      │ 0.01%      │
│ DeConv1      │ 0.006      │ 0.01%      │
│ ReLU_Dec1    │ 0.007      │ 0.01%      │
│ Upsample1    │ 3.98       │ 4.8%       │
│ DeConv2      │ 0.008      │ 0.01%      │
│ ReLU_Dec2    │ 0.007      │ 0.01%      │
│ Upsample2    │ 0.006      │ 0.01%      │
│ FinalConv    │ 0.018      │ 0.02%      │
├──────────────┼────────────┼────────────┤
│ TOTAL        │ 82.25      │ 100%       │
└──────────────┴────────────┴────────────┘

PHÂN TÍCH:

1. CONV1 LÀ BOTTLENECK:
   - Chiếm 85.3% thời gian!
   - Tại sao?
     + 256 output channels (nhiều nhất)
     + 3 input channels
     + 32×32 pixels
     + Mỗi pixel: 3×3×3 = 27 operations
     + Tổng: 32×32×256×27 = 7,077,888 operations!

2. CÁC LAYER KHÁC RẤT NHANH:
   - Conv2, DeConv1, DeConv2, FinalConv: < 0.01ms
   - Tại sao?
     + Kích thước nhỏ hơn (16×16 hoặc 8×8)
     + Ít channels hơn

3. RELU VÀ POOL CŨNG NHANH:
   - ReLU: ~4ms
   - MaxPool, Upsample: ~4ms
   - Đơn giản, ít tính toán

5.3. SO SÁNH VỚI CPU
--------------------

┌─────────────────┬──────────┬──────────┬──────────┐
│ METRIC          │ CPU      │ GPU      │ SPEEDUP  │
├─────────────────┼──────────┼──────────┼──────────┤
│ Cores           │ 4-16     │ 2560     │ 160-640× │
│ Memory BW       │ 50 GB/s  │ 320 GB/s │ 6.4×     │
│ Time (60k imgs) │ 400s     │ 18s      │ 22×      │
│ Time per image  │ 6.7ms    │ 0.3ms    │ 22×      │
│ Conv1 time      │ ~500ms   │ ~70ms    │ 7×       │
└─────────────────┴──────────┴──────────┴──────────┘

TẠI SAO KHÔNG NHANH HƠN?

Lý thuyết: 2560 cores / 16 cores = 160× nhanh hơn
Thực tế: Chỉ 22× nhanh hơn

Nguyên nhân:
1. Direct Convolution không tối ưu
2. Đọc dữ liệu nhiều lần từ global memory
3. Không dùng shared memory
4. Không tận dụng Tensor Cores
5. Overhead của kernel launch

→ CẦN PHASE 3 để tối ưu hóa!

5.4. ƯU ĐIỂM VÀ HẠN CHẾ
------------------------

ƯU ĐIỂM:
✓ Đạt yêu cầu < 20 giây
✓ Nhanh hơn CPU 22 lần
✓ Code đơn giản, dễ hiểu
✓ Nền tảng tốt cho Phase 3

HẠN CHẾ:
✗ Conv1 vẫn là bottleneck (85% thời gian)
✗ Đọc dữ liệu nhiều lần
✗ Không dùng shared memory
✗ Không tận dụng Tensor Cores
✗ Batch size = 1 (chưa tối ưu throughput)

HƯỚNG PHÁT TRIỂN:
→ Phase 3: Im2Col + GEMM
→ Sử dụng shared memory
→ Kernel fusion
→ Batch processing
→ Mục tiêu: < 1 giây cho 60k ảnh!


================================================================================
PHẦN 6: KẾT LUẬN PHASE 2
================================================================================

6.1. TÓM TẮT NHỮNG GÌ ĐÃ LÀM
-----------------------------

Trong Phase 2, nhóm em đã:

1. CHUYỂN ĐỔI TỪ CPU SANG GPU:
   ✓ Viết lại tất cả operations bằng CUDA
   ✓ Quản lý bộ nhớ GPU (malloc, memcpy, free)
   ✓ Tổ chức threads với grid và blocks

2. CÀI ĐẶT DIRECT CONVOLUTION:
   ✓ Mỗi thread tính 1 pixel output
   ✓ 262,144 threads chạy song song
   ✓ Đơn giản nhưng chưa tối ưu

3. CÀI ĐẶT CÁC OPERATIONS KHÁC:
   ✓ ReLU: Hàm kích hoạt phi tuyến
   ✓ MaxPool: Giảm kích thước 2×
   ✓ Upsample: Tăng kích thước 2×

4. ĐẠT KẾT QUẢ:
   ✓ 60,000 ảnh trong 18 giây
   ✓ Nhanh hơn CPU 22 lần
   ✓ Đạt yêu cầu đề bài

6.2. BÀI HỌC RÚT RA
--------------------

1. GPU RẤT MẠNH CHO DEEP LEARNING:
   - Hàng nghìn cores xử lý song song
   - Băng thông memory cao
   - Phù hợp với tính toán số học

2. DIRECT CONVOLUTION CHƯA TỐI ƯU:
   - Đọc dữ liệu nhiều lần
   - Không dùng shared memory
   - Không tận dụng hardware chuyên dụng

3. VẪN CÒN NHIỀU TIỀM NĂNG:
   - Conv1 chiếm 85% thời gian
   - Có thể tối ưu hơn nhiều
   - Phase 3 sẽ giải quyết!

6.3. HƯỚNG PHÁT TRIỂN - PHASE 3
--------------------------------

Các vấn đề cần giải quyết:

VẤN ĐỀ 1: ĐỌC DỮ LIỆU NHIỀU LẦN
→ Giải pháp: Im2Col + GEMM
   - Chuyển Convolution thành Matrix Multiplication
   - Mỗi giá trị chỉ đọc 1 lần

VẤN ĐỀ 2: KHÔNG DÙNG SHARED MEMORY
→ Giải pháp: Tiled GEMM
   - Load tiles vào shared memory
   - Tận dụng tốc độ cao của shared memory

VẤN ĐỀ 3: KHÔNG TẬN DỤNG TENSOR CORES
→ Giải pháp: Matrix Multiplication
   - GEMM có thể dùng Tensor Cores
   - Nhanh hơn 10-100 lần

VẤN ĐỀ 4: KERNEL LAUNCH OVERHEAD
→ Giải pháp: Kernel Fusion + Batch Processing
   - Gộp Conv + ReLU thành 1 kernel
   - Xử lý nhiều ảnh cùng lúc

KẾT QUẢ KỲ VỌNG PHASE 3:
- Thời gian: < 1 giây cho 60k ảnh
- Speedup: 20-30× so với Phase 2
- Speedup: 400-600× so với CPU!


================================================================================
HẾT SCRIPT PHASE 2
================================================================================

CÂU HỎI THƯỜNG GẶP:

Q1: Tại sao không dùng cuDNN?
A: Mục tiêu là hiểu sâu về GPU programming, không chỉ dùng thư viện có sẵn.

Q2: Direct Convolution có ưu điểm gì?
A: Đơn giản, dễ hiểu, dễ debug. Tốt cho học tập và làm baseline.

Q3: Tại sao Conv1 chậm hơn Conv2 nhiều?
A: Conv1 có 256 output channels và kích thước lớn (32×32). Conv2 chỉ 128 
   channels và kích thước nhỏ hơn (16×16).

Q4: Có thể tối ưu Direct Convolution không?
A: Có thể dùng shared memory, nhưng phức tạp và vẫn không bằng Im2Col + GEMM.

Q5: Batch size = 1 có tốt không?
A: Không. Batch size lớn hơn (32, 64) sẽ tận dụng GPU tốt hơn. Phase 3 sẽ 
   dùng batch processing.
